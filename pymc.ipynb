{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "pymc.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "40070007"
      },
      "source": [
        "from datetime import datetime as dt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats as st\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "id": "40070007",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acde30db"
      },
      "source": [
        "### データ読み込み"
      ],
      "id": "acde30db"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2e93a4c"
      },
      "source": [
        "train_kaggle = pd.read_csv('train_kaggle.csv')\n",
        "train_kaggle.datetime = pd.to_datetime(train_kaggle.datetime)\n",
        "train_kaggle.datetime.min(), train_kaggle.datetime.max()"
      ],
      "id": "f2e93a4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8f66011"
      },
      "source": [
        " ### 目的変数をドルに変換"
      ],
      "id": "b8f66011"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e34eda2a",
        "scrolled": true
      },
      "source": [
        "minimum_unit = train_kaggle.operation_value.abs().min()\n",
        "train_kaggle['target'] = train_kaggle.operation_value / minimum_unit\n",
        "train_kaggle['target'] = train_kaggle['target'].astype(int)\n",
        "train_kaggle.head()"
      ],
      "id": "e34eda2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9a84be"
      },
      "source": [
        "### ヒストグラム"
      ],
      "id": "af9a84be"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fb23336"
      },
      "source": [
        "print(train_kaggle.target.mean(),train_kaggle.target.min(),train_kaggle.target.max(),)\n",
        "train_kaggle.target.hist(bins=30);"
      ],
      "id": "8fb23336",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auxTugrvqNDD"
      },
      "source": [
        "for atm_id in by_atm.keys():\n",
        "    idx = by_atm[atm_id].datetime\n",
        "    x = by_atm[atm_id].target\n",
        "    plt.plot(idx, x);"
      ],
      "id": "auxTugrvqNDD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc61f8ac"
      },
      "source": [
        "\n",
        "\n",
        "### ヒートマップ"
      ],
      "id": "fc61f8ac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6befbd2"
      },
      "source": [
        "train_kaggle['dayofweek'] = train_kaggle.datetime.dt.dayofweek\n",
        "train_kaggle['day'] = train_kaggle.datetime.dt.day\n",
        "train_kaggle['month'] = train_kaggle.datetime.dt.month\n",
        "train_kaggle['week'] = train_kaggle.datetime.dt.week\n",
        "sns.heatmap(train_kaggle.pivot_table(index='day', columns='dayofweek', values='target'));"
      ],
      "id": "a6befbd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f27dc750"
      },
      "source": [
        "### ATM 別のトランザクション数"
      ],
      "id": "f27dc750"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fea47794"
      },
      "source": [
        "\n",
        "print(len(train_kaggle.atm_id.unique()))\n",
        "train_kaggle.atm_id.value_counts()[[ 74,  85,  86,  87,  88,  90,  91,  93,  94,  96,  97,  98,  99,\n",
        "            100, 101, 102, 103, 104, 105, 106]].plot(kind='barh', figsize=(10, 10));"
      ],
      "id": "fea47794",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8f635cb"
      },
      "source": [
        "### 前処理"
      ],
      "id": "e8f635cb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57db8b1c"
      },
      "source": [
        "def read_weather_data(path):\n",
        "    \"\"\"\n",
        "    pathのデフォルトは 'dailydata.csv'\n",
        "    \"\"\"\n",
        "    weather_data = pd.read_csv(path, encoding='shift_jis')\n",
        "    weather_data.datetime = pd.to_datetime(weather_data.datetime)\n",
        "    weather_data = weather_data.drop(columns=['年', '月', '日'])\n",
        "    return weather_data\n",
        "\n",
        "def merge_weather_data(data, weather_data):\n",
        "    return pd.merge(data, weather_data, how='inner', on='datetime')\n",
        "    \n",
        "    \n",
        "\n",
        "def sumrize_daily_data(data):\n",
        "        daily_data = data.pivot_table(\n",
        "                                index=['datetime', 'atm_id', 'month', 'dayofweek', 'day'], \n",
        "                                values='target',\n",
        "                                aggfunc='sum').reset_index()\n",
        "        return daily_data\n",
        "\n",
        "def add_poly_feature(data, target_feature, dim):\n",
        "    \"\"\"\n",
        "    多項式回帰用: 指定したfeatureを次数dimまでの特徴量を追加する\n",
        "    \"\"\"\n",
        "    if dim < 2:\n",
        "        return data\n",
        "    else:\n",
        "        poly_feature = np.vstack([data[target_feature]**i for i in range(1, dim)]).T\n",
        "        feature_names = [f'{target_feature}^{i}' for i in range(2, dim)]\n",
        "        for i, feature_name in enumerate(feature_names):\n",
        "            data[feature_name] = poly_feature[:, i]\n",
        "        return data\n",
        "\n",
        "def add_passed_date_feature(data, datefrom):\n",
        "    \"\"\"\n",
        "    datefrom からの経過日数特徴量を追加する\n",
        "    フォーマッt : YYYY-mm-dd\n",
        "    \"\"\"\n",
        "    datefrom = dt.strptime(datefrom, '%Y-%m-%d')\n",
        "    return (data.datetime - datefrom).dt.days\n",
        "\n",
        "def add_salary10th(data):\n",
        "    return data['day'].replace({10: 1, 1: 0, 2: 0, 3: 0, 4: 0\n",
        "                                                   , 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 11: 0\n",
        "                                                   , 12: 0, 13: 0, 14: 0, 15: 0, 16: 0\n",
        "                                                   , 17: 0, 18: 0, 19: 0, 20: 0, 21: 0\n",
        "                                                   , 22: 0, 23: 0, 24: 0, 25: 0, 26: 0\n",
        "                                                   , 27: 0, 28: 0, 29: 0, 30: 0, 31: 0})\n",
        "\n",
        "def add_salary25th(data):\n",
        "    return data['day'].replace({25: 1, 1: 0, 2: 0, 3: 0, 4: 0\n",
        "                                                   , 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 11: 0\n",
        "                                                   , 12: 0, 13: 0, 14: 0, 15: 0, 16: 0\n",
        "                                                   , 17: 0, 18: 0, 19: 0, 20: 0, 21: 0\n",
        "                                                   , 22: 0, 23: 0, 24: 0, 10: 0, 26: 0\n",
        "                                                   , 27: 0, 28: 0, 29: 0, 30: 0, 31: 0})\n",
        "\n",
        "\n",
        "def preprocess(data):\n",
        "    daily_data = sumrize_daily_data(data)\n",
        "    weather_data = read_weather_data('dailydata.csv')\n",
        "    daily_data = merge_weather_data(daily_data, weather_data)\n",
        "    \n",
        "    daily_data['passed_date'] = add_passed_date_feature(daily_data, '2017-06-09')\n",
        "    # , :\n",
        "    for col in ['passed_date', 'day', '日平均気温(℃)', '日最高気温(℃)', '日最低気温(℃)', '日降水量(mm)']:\n",
        "        daily_data = add_poly_feature(daily_data, col, 5)\n",
        "        \n",
        "    daily_data['salary10th'] = add_salary10th(daily_data)\n",
        "    daily_data['salary25th'] = add_salary25th(daily_data)    \n",
        "    daily_data = pd.get_dummies(data=daily_data, columns=['dayofweek', 'month'], drop_first=True)\n",
        "    return daily_data\n",
        "\n",
        "\n",
        "def get_feature_target_cols(data):\n",
        "    \n",
        "    target_col = 'target'\n",
        "    exclude_cols = ['target', 'datetime', 'client_id', 'operation_value', 'atm_id', 'operation_type']\n",
        "    feature_cols = [col for col in data.columns if col not in exclude_cols]  # data_preprocessed には前処理ずみのデータを入れる\n",
        "    \n",
        "    return feature_cols, target_col\n",
        "\n",
        "\n",
        "def split_by_atm(data):\n",
        "    dataset_by_atm = {}\n",
        "    for atm_id in data.atm_id.unique(): \n",
        "        is_target = atm_id == data.atm_id\n",
        "        data_by_atm = data[is_target].reset_index(drop=True) \n",
        "        dataset_by_atm[atm_id] = data_by_atm\n",
        "    return dataset_by_atm\n",
        "\n",
        "\n",
        "def read_atm_data(path):\n",
        "    data = pd.read_csv(path)\n",
        "    data.datetime = pd.to_datetime(data.datetime)\n",
        "    return data\n",
        "\n",
        "\n",
        "def to_dollar(data):    \n",
        "    minimum_unit = data.operation_value.abs().min()\n",
        "    dollar = data.operation_value / minimum_unit\n",
        "    dollar = dollar.astype(int)\n",
        "    return dollar\n",
        "\n",
        "\n",
        "def add_date_features(data):\n",
        "    data['dayofweek'] = data.datetime.dt.dayofweek\n",
        "    data['day'] = data.datetime.dt.day\n",
        "    data['month'] = data.datetime.dt.month\n",
        "#     data['week'] = data.datetime.dt.week\n",
        "    return data\n",
        "\n",
        "def data_preparation_main(path='train_kaggle.csv'):\n",
        "    data = read_atm_data(path)\n",
        "    data['target'] = to_dollar(data)\n",
        "    data = add_date_features(data)\n",
        "    preprocessed = preprocess(data)\n",
        "    by_atm = split_by_atm(preprocessed)\n",
        "    feature_cols, target_col = get_feature_target_cols(preprocessed)\n",
        "    return preprocessed, by_atm, feature_cols,target_col"
      ],
      "id": "57db8b1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7edac547"
      },
      "source": [
        "### pymc"
      ],
      "id": "7edac547"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deba2c1e"
      },
      "source": [
        "import scipy.stats as st\n",
        "\n",
        "import pymc3 as pm\n",
        "import arviz as az\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "id": "deba2c1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24e6aba3"
      },
      "source": [
        "by_atm[87].shape"
      ],
      "id": "24e6aba3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee2d9bab"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "train = by_atm[87]\n",
        "X = train[feature_cols].values\n",
        "y = train[target_col].values\n",
        "\n",
        "lr = Lasso(alpha=1)\n",
        "lr.fit(X, y)\n",
        "plt.hist(lr.coef_);"
      ],
      "id": "ee2d9bab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7cd905e"
      },
      "source": [
        "def get_date_bucket(x):\n",
        "    if x < 10:\n",
        "        return 'month_head'\n",
        "    elif x < 20:\n",
        "        return 'month_mid'\n",
        "    else:\n",
        "        return 'month_end'"
      ],
      "id": "c7cd905e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87bfbe68"
      },
      "source": [
        "feature_for_sd = [\n",
        " 'passed_date', \n",
        " 'dayofweek_1',\n",
        " 'dayofweek_2',\n",
        " 'dayofweek_3',\n",
        " 'dayofweek_4',\n",
        " 'dayofweek_5',\n",
        " 'dayofweek_6',\n",
        " 'month_2',\n",
        " 'month_3',\n",
        " 'month_4',\n",
        " 'month_5',\n",
        " 'month_6',\n",
        " 'month_7',\n",
        " 'month_8',\n",
        " 'month_9',\n",
        " 'month_10',\n",
        " 'month_11',\n",
        " 'month_12',\n",
        " 'salary10th',\n",
        " 'salary25th',\n",
        " 'month_head',\n",
        " 'month_mid',\n",
        " 'month_end']"
      ],
      "id": "87bfbe68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d130929"
      },
      "source": [
        "train = by_atm[87]\n",
        "X = train[feature_cols].values\n",
        "y = train[target_col].values\n",
        "\n",
        "train['month_head'] = (train['day'] <= 10).astype(int)\n",
        "train['month_mid'] = ((10 < train['day'])&(train['day'] <= 20)).astype(int)\n",
        "train['month_end'] = (20 < train['day']).astype(int)    \n",
        "\n",
        "X_for_sd = train[feature_for_sd].values\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_for_sd = scaler.fit_transform(X_for_sd)\n",
        "\n",
        "X = scaler.fit_transform(X)"
      ],
      "id": "3d130929",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbb521e3"
      },
      "source": [
        "N = train['passed_date'].max()\n",
        "date = train['passed_date'].values"
      ],
      "id": "dbb521e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "304bef38"
      },
      "source": [
        "with pm.Model() as model:    \n",
        "    # 回帰係数\n",
        "    mu_w_1 = pm.Uniform('mu_w_1', lower=-1000, upper=1000, shape=len(feature_cols))\n",
        "    mu_w_2 = pm.Uniform('mu_w_2', lower=-1000, upper=1000, shape=len(feature_cols))\n",
        "    \n",
        "    # 切片\n",
        "    mu_intercept_1 = pm.Uniform('mu_intercept_1', lower=-1000, upper=1000,)\n",
        "    mu_intercept_2 = pm.Uniform('mu_intercept_2', lower=-1000, upper=1000,)\n",
        "    \n",
        "    # y = α + βX\n",
        "    mu_1 = pm.math.dot(X, mu_w_1) + mu_intercept_1\n",
        "    mu_2 = pm.math.dot(X, mu_w_2) + mu_intercept_2\n",
        "    \n",
        "    #  y の変化点\n",
        "    mu_tau = pm.DiscreteUniform('mu_tau', lower=0, upper=N)\n",
        "    mu = pm.Deterministic('mu', pm.math.switch(mu_tau > date, mu_1, mu_2))\n",
        "        \n",
        "    # 分散の重み\n",
        "    sd_w_1 = pm.Uniform('sd_w_1', lower=-1000, upper=1000, shape=len(feature_for_sd))\n",
        "    sd_w_2 = pm.Uniform('sd_w_2', lower=-1000, upper=1000, shape=len(feature_for_sd))\n",
        "    sd_intercept_1 = pm.Uniform('sd_intercept_1', lower=-1000, upper=1000)\n",
        "    sd_intercept_2 = pm.Uniform('sd_intercept_2', lower=-1000, upper=1000)\n",
        "    \n",
        "    # 分散を線形結合で求める\n",
        "    sd_1 = pm.math.dot(X_for_sd, sd_w_1) + sd_intercept_1\n",
        "    sd_2 = pm.math.dot(X_for_sd, sd_w_2) + sd_intercept_2\n",
        "    \n",
        "    #  sdの変化点\n",
        "    sd_tau = pm.DiscreteUniform('sd_tau', lower=0, upper=N)\n",
        "    sd = pm.Deterministic('sd', pm.math.switch(sd_tau > date, sd_1, sd_2))\n",
        "    \n",
        "    pm_y = pm.Normal('pm_y', mu=mu, sd=pm.math.exp(sd), observed=y)"
      ],
      "id": "304bef38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7444f7e6"
      },
      "source": [
        "with model:\n",
        "    trace = pm.sample(draws=1000, chains=1, step=pm.Metropolis())"
      ],
      "id": "7444f7e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0f058e0"
      },
      "source": [
        "trace.varnames"
      ],
      "id": "b0f058e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec3b9215"
      },
      "source": [
        "az.summary(trace, var_names=['mu_tau', 'sd_tau'])"
      ],
      "id": "ec3b9215",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6bf456b"
      },
      "source": [
        "coef_result = az.summary(trace, var_names=['mu_w_1', 'mu_w_2'])\n",
        "coef_result['feature_name'] = feature_cols * 2\n",
        "coef_result"
      ],
      "id": "a6bf456b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44296f2f"
      },
      "source": [
        "az.summary(trace, var_names=['sd_w_1', 'sd_w_2'])"
      ],
      "id": "44296f2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6f30b7"
      },
      "source": [
        "az.plot_trace(trace, var_names=['mu_tau', 'sd_tau']);"
      ],
      "id": "ff6f30b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c50059c"
      },
      "source": [
        "sd_w_dataframe = pd.DataFrame(index=feature_for_sd, data={'sd_before': trace['sd_w_1'].mean(axis=0), 'sd_after':trace['sd_w_2'].mean(axis=0)})\n",
        "sd_w_dataframe.plot(kind='barh', figsize=(20, 10));"
      ],
      "id": "0c50059c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22e4dc28"
      },
      "source": [
        "coef_w_dataframe = pd.DataFrame(index=feature_cols, data={'coef_before': trace['mu_w_1'].mean(axis=0), 'coef_after':trace['mu_w_2'].mean(axis=0)})\n",
        "coef_w_dataframe.plot(kind='barh', figsize=(20, 10));\n"
      ],
      "id": "22e4dc28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c37a4f1"
      },
      "source": [
        "sd_tau_matrix = np.ones([len(date), len(trace['sd_tau'])])\n",
        "sd_tau = ((sd_tau_matrix * trace['sd_tau']).T > date).mean(axis=0)\n",
        "\n",
        "sd_1 = X_for_sd @ trace['sd_w_1'].mean(axis=0) + trace['sd_intercept_1'].mean()\n",
        "sd_2 = X_for_sd @ trace['sd_w_2'].mean(axis=0) + trace['sd_intercept_2'].mean()\n",
        "\n",
        "sd_before = sd_1 * sd_tau\n",
        "sd_after = sd_2 * (1 - sd_tau)\n",
        "\n",
        "sd = sd_before + sd_after\n",
        "# sd"
      ],
      "id": "0c37a4f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2aaa04b"
      },
      "source": [
        "mu_tau_matrix = np.ones([len(date), len(trace['mu_tau'])])\n",
        "mu_tau = ((mu_tau_matrix * trace['mu_tau']).T > date).mean(axis=0)\n",
        "\n",
        "mu_1 = X @ trace['mu_w_1'].mean(axis=0) + trace['mu_intercept_1'].mean()\n",
        "mu_2 = X @ trace['mu_w_2'].mean(axis=0) + trace['mu_intercept_2'].mean()\n",
        "\n",
        "mu_before = mu_1 * mu_tau\n",
        "mu_after = mu_2 * (1 - mu_tau)\n",
        "\n",
        "mu = mu_before + mu_after\n",
        "# mu"
      ],
      "id": "b2aaa04b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7357d364"
      },
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(train.datetime, train.target)\n",
        "plt.plot(train.datetime, mu)\n",
        "\n",
        "plt.fill_between(train.datetime, mu + (1.96 * np.exp(sd)), mu - (1.96 * np.exp(sd)), alpha=0.4);"
      ],
      "id": "7357d364",
      "execution_count": null,
      "outputs": []
    }
  ]
}