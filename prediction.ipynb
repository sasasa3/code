{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kaggle = pd.read_csv('train_kaggle.csv')\n",
    "train_kaggle.datetime = pd.to_datetime(train_kaggle.datetime)\n",
    "train_kaggle.datetime.min(), train_kaggle.datetime.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目的変数をドルに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minimum_unit = train_kaggle.operation_value.abs().min()\n",
    "train_kaggle['target'] = (train_kaggle.operation_value / minimum_unit).astype(int)\n",
    "train_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ヒストグラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_kaggle.target.mean(),train_kaggle.target.min(),train_kaggle.target.max(),)\n",
    "train_kaggle.target.hist(bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_atm = {}\n",
    "for atm_id in train_kaggle.atm_id.unique():\n",
    "    by_target = atm_id == train_kaggle.atm_id \n",
    "    byatm = train_kaggle[by_target].reset_index(drop=True) \n",
    "    by_atm[atm_id] = byatm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ヒートマップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kaggle['dayofweek'] = train_kaggle.datetime.dt.dayofweek\n",
    "train_kaggle['day'] = train_kaggle.datetime.dt.day\n",
    "train_kaggle['month'] = train_kaggle.datetime.dt.month\n",
    "train_kaggle['week'] = train_kaggle.datetime.dt.week\n",
    "sns.heatmap(train_kaggle.pivot_table(index='day', columns='dayofweek', values='target'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATM別のデータ件数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(train_kaggle.atm_id.unique()))\n",
    "train_kaggle.atm_id.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = train_kaggle.pivot_table(\n",
    "    index=['datetime', 'atm_id', 'month', 'week', 'dayofweek', 'day'], \n",
    "    values='target',\n",
    "    aggfunc='sum').reset_index();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed['month*2'] = data_preprocessed['month'] ** 2\n",
    "data_preprocessed['month*3'] = data_preprocessed['month'] ** 3\n",
    "data_preprocessed['month*4'] = data_preprocessed['month'] ** 4\n",
    "data_preprocessed['week*2'] = data_preprocessed['week'] ** 2\n",
    "data_preprocessed['week*3'] = data_preprocessed['week'] ** 3\n",
    "data_preprocessed['week*4'] = data_preprocessed['week'] ** 4\n",
    "print(data_preprocessed.shape)\n",
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = pd.get_dummies(data_preprocessed, columns=['dayofweek'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#給料日の特徴量を追加\n",
    "\n",
    "data_preprocessed['salary10th']=train_kaggle['day'].replace({10: 1, 1: 0, 2: 0, 3: 0, 4: 0\n",
    "                                                   , 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 11: 0\n",
    "                                                   , 12: 0, 13: 0, 14: 0, 15: 0, 16: 0\n",
    "                                                   , 17: 0, 18: 0, 19: 0, 20: 0, 21: 0\n",
    "                                                   , 22: 0, 23: 0, 24: 0, 25: 0, 26: 0\n",
    "                                                   , 27: 0, 28: 0, 29: 0, 30: 0, 31: 0})\n",
    "\n",
    "data_preprocessed['salary25th']=train_kaggle['day'].replace({25: 1, 1: 0, 2: 0, 3: 0, 4: 0\n",
    "                                                   , 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 11: 0\n",
    "                                                   , 12: 0, 13: 0, 14: 0, 15: 0, 16: 0\n",
    "                                                   , 17: 0, 18: 0, 19: 0, 20: 0, 21: 0\n",
    "                                                   , 22: 0, 23: 0, 24: 0, 10: 0, 26: 0\n",
    "                                                   , 27: 0, 28: 0, 29: 0, 30: 0, 31: 0})\n",
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailydata = pd.read_csv('daily_data.csv', encoding='shift_jis')\n",
    "dailydata.datetime = pd.to_datetime(dailydata.datetime)\n",
    "dailydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailydata = dailydata.drop('年',axis=1)\n",
    "dailydata = dailydata.drop('月',axis=1)\n",
    "dailydata = dailydata.drop('日',axis=1)\n",
    "dailydata['average_temp(℃)'] = dailydata['日平均気温(℃)'].astype(int)\n",
    "dailydata['max_temp(℃)'] = dailydata['日最高気温(℃)'].astype(int)\n",
    "dailydata['min_temp(℃)'] = dailydata['日最低気温(℃)'].astype(int)\n",
    "dailydata['日降水量(mm)'].fillna(0, inplace=True)\n",
    "dailydata['precipitation(mm)'] = dailydata['日降水量(mm)'].astype(int)\n",
    "dailydata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = data_preprocessed.merge(dailydata, on='datetime', how='left')\n",
    "data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed.plot(kind='scatter', x='average_temp(℃)', y='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed.plot(kind='scatter', x='max_temp(℃)', y='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed.plot(kind='scatter', x='min_temp(℃)', y='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed.plot(kind='scatter', x='precipitation(mm)', y='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'target'\n",
    "exclude_cols = ['target', 'datetime', 'client_id', 'operation_value', 'atm_id', 'operation_type']\n",
    "feature_cols = [col for col in data_preprocessed.columns if col not in exclude_cols]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 標準化を行う\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data_preprocessed[feature_cols])\n",
    "for i, col in enumerate(feature_cols):\n",
    "    data_preprocessed[col] = X[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_by_atm = {}\n",
    "for atm_id in data_preprocessed.atm_id.unique():\n",
    "    is_target = atm_id == data_preprocessed.atm_id \n",
    "    is_notnull = ~data_preprocessed['日平均気温(℃)'].isnull()\n",
    "    data_by_atm = data_preprocessed[(is_target)&(is_notnull)].reset_index(drop=True)\n",
    "    dataset_by_atm[atm_id] = data_by_atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_atm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_by_atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "ratio_date = '2018-08-01 00:00:00'\n",
    "ratio_date = dt.strptime(ratio_date, '%Y-%m-%d %H:%M:%S')\n",
    "ratio_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import timedelta\n",
    "train_start_date = ratio_date - timedelta(days=243)\n",
    "train_start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形回帰\n",
    "\n",
    "score_by_atm = {}\n",
    "predict_by_atm = {}\n",
    "target_by_atm = {}\n",
    "date_by_atm = {}\n",
    "models = {}\n",
    "\n",
    "\n",
    "for atm_id in dataset_by_atm.keys():\n",
    "    \n",
    "    target_data = dataset_by_atm[atm_id]\n",
    "    is_test = target_data.datetime >= ratio_date\n",
    "    test = target_data[is_test].sort_values(by='datetime')\n",
    "    is_train_start = target_data.datetime >= train_start_date\n",
    "    train = target_data[(~is_test) & (is_train_start)]\n",
    "    \n",
    "    if len(test) == 0:\n",
    "        print(atm_id)\n",
    "        break\n",
    "\n",
    "    test_X = test[feature_cols]\n",
    "    train_X = train[feature_cols]\n",
    "\n",
    "    test_y = test[target_col]\n",
    "    train_y = train[target_col]\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train_X, train_y)\n",
    "    pred = lr.predict(test_X)\n",
    "    mse = mean_squared_error(pred, test_y)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    models[atm_id] = *lr.coef_, lr.intercept_\n",
    "    score_by_atm[atm_id] = rmse\n",
    "    predict_by_atm[atm_id] = pred\n",
    "    target_by_atm[atm_id] = test_y\n",
    "    date_by_atm[atm_id] = test.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 線形回帰\n",
    "plt.bar(score_by_atm.keys(), list(score_by_atm.values()));\n",
    "np.mean(list(score_by_atm.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_atm.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_atm.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形回帰\n",
    "atm_id = 74\n",
    "print(score_by_atm[atm_id])\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(date_by_atm[atm_id], target_by_atm[atm_id], label='test')\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id], label='pred')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 線形回帰\n",
    "atm_id = 87\n",
    "print(score_by_atm[atm_id])\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(date_by_atm[atm_id], target_by_atm[atm_id], label='test')\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id], label='pred')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### リッジ回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リッジ回帰\n",
    "\n",
    "score_by_atm = {}\n",
    "predict_by_atm = {}\n",
    "target_by_atm = {}\n",
    "date_by_atm = {}\n",
    "models = {}\n",
    "\n",
    "\n",
    "for atm_id in dataset_by_atm.keys():\n",
    "    \n",
    "    target_data = dataset_by_atm[atm_id]\n",
    "    is_test = target_data.datetime >= ratio_date\n",
    "    test = target_data[is_test].sort_values(by='datetime')\n",
    "    is_train_start = target_data.datetime >= train_start_date\n",
    "    train = target_data[(~is_test) & (is_train_start)]\n",
    "    \n",
    "    if len(test) == 0:\n",
    "        print(atm_id)\n",
    "        break\n",
    "\n",
    "    test_X = test[feature_cols]\n",
    "    train_X = train[feature_cols]\n",
    "\n",
    "    test_y = test[target_col]\n",
    "    train_y = train[target_col]\n",
    "\n",
    "    ridge = Ridge(alpha=5)\n",
    "    ridge.fit(train_X, train_y)\n",
    "    pred = ridge.predict(test_X)\n",
    "    mse = mean_squared_error(pred, test_y)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    models[atm_id] = *ridge.coef_, ridge.intercept_\n",
    "    score_by_atm[atm_id] = rmse\n",
    "    predict_by_atm[atm_id] = pred\n",
    "    target_by_atm[atm_id] = test_y\n",
    "    date_by_atm[atm_id] = test.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　特徴量ごとの回帰係数＋切片\n",
    "columns = *feature_cols, 'intercept'\n",
    "coef_matrix = pd.DataFrame(models, index=columns).T\n",
    "coef_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　リッジ回帰\n",
    "\n",
    "plt.bar(score_by_atm.keys(), list(score_by_atm.values()));\n",
    "np.mean(list(score_by_atm.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_atm.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_by_atm.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　リッジ回帰\n",
    "atm_id = 74\n",
    "print(score_by_atm[atm_id])\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(date_by_atm[atm_id], target_by_atm[atm_id], label='test')\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id], label='pred')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "atm_id = 87\n",
    "print(score_by_atm[atm_id])\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(date_by_atm[atm_id], target_by_atm[atm_id], label='test')\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id], label='pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_atm = 87\n",
    "\n",
    "sorted_data = data_preprocessed.sort_values(by='datetime')\n",
    "is_target = sorted_data.atm_id == target_atm \n",
    "date = sorted_data[is_target].datetime\n",
    "target = sorted_data[is_target].target\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(date, target);\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムフォレスト\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "score_by_atm = {}\n",
    "predict_by_atm = {}\n",
    "target_by_atm = {}\n",
    "date_by_atm = {}\n",
    "models = {}\n",
    "\n",
    "\n",
    "for atm_id in dataset_by_atm.keys():\n",
    "    \n",
    "    target_data = dataset_by_atm[atm_id]\n",
    "    is_test = target_data.datetime >= ratio_date\n",
    "    test = target_data[is_test].sort_values(by='datetime')\n",
    "    is_train_start = target_data.datetime >= train_start_date\n",
    "    train = target_data[(~is_test) & (is_train_start)]\n",
    "    \n",
    "    if len(test) == 0:\n",
    "        print(atm_id)\n",
    "        break\n",
    "\n",
    "    test_X = test[feature_cols]\n",
    "    train_X = train[feature_cols]\n",
    "\n",
    "    test_y = test[target_col]\n",
    "    train_y = train[target_col]\n",
    "\n",
    "    rf = RandomForestRegressor(random_state=1234)\n",
    "    rf.fit(train_X, train_y)\n",
    "    pred = rf.predict(test_X)\n",
    "    mse = mean_squared_error(test_y, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    models[atm_id] = *ridge.coef_, ridge.intercept_\n",
    "    score_by_atm[atm_id] = rmse\n",
    "    predict_by_atm[atm_id] = pred\n",
    "    target_by_atm[atm_id] = test_y\n",
    "    date_by_atm[atm_id] = test.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　ランダムフォレスト\n",
    "\n",
    "plt.bar(score_by_atm.keys(), list(score_by_atm.values()));\n",
    "np.mean(list(score_by_atm.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_atm.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_atm.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　ランダムフォレスト\n",
    "atm_id = 74\n",
    "print(score_by_atm[atm_id])\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(date_by_atm[atm_id], target_by_atm[atm_id], label='test')\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id], label='pred')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#　ランダムフォレスト\n",
    "atm_id = 87\n",
    "print(score_by_atm[atm_id])\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(date_by_atm[atm_id], target_by_atm[atm_id], label='test')\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id], label='pred')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed2 = data_preprocessed\n",
    "data_preprocessed2 = data_preprocessed2[['datetime' ,'target','atm_id']]\n",
    "data_preprocessed2 = data_preprocessed2.rename(columns={'datetime':'ds', 'target':'y'})\n",
    "data_preprocessed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_p = ['y']\n",
    "feature_cols_p = ['ds','y']\n",
    "feature_cols_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_by_atm = {}\n",
    "for atm_id in data_preprocessed2.atm_id.unique():\n",
    "    is_target = atm_id == data_preprocessed2.atm_id \n",
    "    data_by_atm = data_preprocessed2[is_target].reset_index(drop=True)\n",
    "    dataset_by_atm[atm_id] = data_by_atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_atm = {}\n",
    "predict_by_atm = {}\n",
    "target_by_atm = {}\n",
    "date_by_atm = {}\n",
    "models = {}\n",
    "\n",
    "\n",
    "for atm_id in dataset_by_atm.keys():\n",
    "    \n",
    "    target_data = dataset_by_atm[atm_id]\n",
    "    is_test = target_data.ds >= ratio_date\n",
    "    test = target_data[is_test].sort_values(by='ds')\n",
    "    is_train_start = target_data.ds >= train_start_date\n",
    "    train = target_data[(~is_test) & (is_train_start)]\n",
    "    \n",
    "    if len(test) == 0:\n",
    "        print(atm_id)\n",
    "        break\n",
    "\n",
    "    test_X = test[feature_cols_p]\n",
    "    train_X = train[feature_cols_p]\n",
    "\n",
    "    test_y = test[target_col_p]\n",
    "    train_y = train[target_col_p]\n",
    "\n",
    "\n",
    "    m = Prophet()\n",
    "    m.fit(train_X)\n",
    "    \n",
    "    pred = m.predict(test_X)\n",
    "    pred = pred['yhat']\n",
    "    \n",
    "    mse = mean_squared_error(pred, test_y)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    score_by_atm[atm_id] = rmse\n",
    "    predict_by_atm[atm_id] = pred\n",
    "    target_by_atm[atm_id] = test_y\n",
    "    date_by_atm[atm_id] = test.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(score_by_atm.keys(), list(score_by_atm.values()));\n",
    "np.mean(list(score_by_atm.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_atm.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_atm.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "atm_id = 74\n",
    "print(score_by_atm[atm_id])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(date_by_atm[atm_id], target_by_atm[atm_id], label='test')\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id], label='pred')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_id = 87\n",
    "print(score_by_atm[atm_id])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(date_by_atm[atm_id], target_by_atm[atm_id], label='test')\n",
    "plt.plot(date_by_atm[atm_id], predict_by_atm[atm_id], label='pred')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATMグルーピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = *feature_cols, 'intercept'\n",
    "coef_matrix = pd.DataFrame(models, index=columns).T\n",
    "coef_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 標準化を行う\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(coef_matrix)\n",
    "coef_matrix2 = scaler.transform(coef_matrix)\n",
    "coef_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix2 = (coef_matrix - coef_matrix.mean()) / coef_matrix.std()\n",
    "coef_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# データフレームの各列を正規化\n",
    "coef_matrix3 = coef_matrix.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "coef_matrix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeansで似た傾向のatm_idを可視化\n",
    "%matplotlib notebook\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = coef_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを初期化\n",
    "km =KMeans(n_clusters=3, random_state=1234)\n",
    "\n",
    "km.fit(data)\n",
    "\n",
    "cluster_label = km.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "data_with_cluster_label = copy.copy(data)\n",
    "data_with_cluster_label['cluster_label'] = cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "\n",
    "for i  in range(1,15):                # 1~14クラスタまで計算 \n",
    "    km = KMeans(n_clusters=i, random_state=1234)\n",
    "    km.fit(data)                       # クラスタリングの計算を実行\n",
    "    distortions.append(km.inertia_)   \n",
    "\n",
    "plt.plot(range(1,15),distortions,marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1回目グルーピング\n",
    "coef_matrix_cluster1 = coef_matrix2[['week' ,'day']]\n",
    "coef_matrix_cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_array= np.array([\n",
    "    coef_matrix_cluster1['week'].tolist(),\n",
    "    coef_matrix_cluster1['day'].tolist()\n",
    "    ], np.float)\n",
    "c_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_array = c_array.T\n",
    "print(c_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "clf = KMeans(n_clusters = num_clusters ) \n",
    "clf.fit(c_array) \n",
    "pred = clf.predict(c_array) \n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_cluster1['atm_cluster_id'] = pred\n",
    "coef_matrix_cluster1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "colors = ['Red', 'Blue', 'Green']\n",
    " \n",
    "for t in range(num_clusters):\n",
    "    x = coef_matrix_cluster1.loc[coef_matrix_cluster1['atm_cluster_id'] == t, 'week']\n",
    "    y = coef_matrix_cluster1.loc[coef_matrix_cluster1['atm_cluster_id'] == t, 'day']\n",
    " \n",
    "    ax.scatter(x, y, alpha=0.5, label='cluster ' + str(t), color=colors[t])\n",
    " \n",
    "ax.set_title('Scatter Plot')\n",
    "ax.set_xlabel('week')\n",
    "ax.set_ylabel('day')\n",
    " \n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2回目グルーピング\n",
    "coef_matrix_cluster2 = coef_matrix2[['intercept' ,'week']]\n",
    "coef_matrix_cluster2\n",
    "\n",
    "c_array= np.array([\n",
    "    coef_matrix_cluster2['week'].tolist(),\n",
    "    coef_matrix_cluster2['intercept'].tolist()\n",
    "    ], np.float)\n",
    "\n",
    "c_array = c_array.T\n",
    "\n",
    "num_clusters = 3\n",
    "clf = KMeans(n_clusters = num_clusters ) \n",
    "clf.fit(c_array) \n",
    "pred = clf.predict(c_array) \n",
    "\n",
    "coef_matrix_cluster2['atm_cluster_id'] = pred\n",
    "coef_matrix_cluster2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, )\n",
    "colors = ['Red', 'Blue', 'Green']\n",
    " \n",
    "for t in range(num_clusters):\n",
    "    x = coef_matrix_cluster2.loc[coef_matrix_cluster2['atm_cluster_id'] == t, 'week']\n",
    "    y = coef_matrix_cluster2.loc[coef_matrix_cluster2['atm_cluster_id'] == t, 'intercept']\n",
    " \n",
    "    ax.scatter(x, y, alpha=0.5, label='cluster ' + str(t), color=colors[t])\n",
    " \n",
    "ax.set_title('Scatter Plot')\n",
    "ax.set_xlabel('week')\n",
    "ax.set_ylabel('intercept')\n",
    " \n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3回目グルーピング\n",
    "coef_matrix_cluster3 = coef_matrix2[['intercept' ,'day']]\n",
    "coef_matrix_cluster3\n",
    "\n",
    "c_array= np.array([\n",
    "    coef_matrix_cluster3['day'].tolist(),\n",
    "    coef_matrix_cluster3['intercept'].tolist()\n",
    "    ], np.float)\n",
    "\n",
    "c_array = c_array.T\n",
    "\n",
    "num_clusters = 3\n",
    "clf = KMeans(n_clusters = num_clusters ) \n",
    "clf.fit(c_array) \n",
    "pred = clf.predict(c_array) \n",
    "\n",
    "coef_matrix_cluster3['atm_cluster_id'] = pred\n",
    "coef_matrix_cluster3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, )\n",
    "colors = ['Red', 'Blue', 'Green']\n",
    " \n",
    "for t in range(num_clusters):\n",
    "    x = coef_matrix_cluster3.loc[coef_matrix_cluster3['atm_cluster_id'] == t, 'day']\n",
    "    y = coef_matrix_cluster3.loc[coef_matrix_cluster3['atm_cluster_id'] == t, 'intercept']\n",
    " \n",
    "    ax.scatter(x, y, alpha=0.5, label='cluster ' + str(t), color=colors[t])\n",
    " \n",
    "ax.set_title('Scatter Plot')\n",
    "ax.set_xlabel('day')\n",
    "ax.set_ylabel('intercept')\n",
    " \n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4回目グルーピング\n",
    "coef_matrix_cluster4 = coef_matrix2[['intercept' ,'salary25th']]\n",
    "coef_matrix_cluster4\n",
    "\n",
    "c_array= np.array([\n",
    "    coef_matrix_cluster4['intercept'].tolist(),\n",
    "    coef_matrix_cluster4['salary25th'].tolist()\n",
    "    ], np.float)\n",
    "\n",
    "c_array = c_array.T\n",
    "\n",
    "num_clusters = 3\n",
    "clf = KMeans(n_clusters = num_clusters ) \n",
    "clf.fit(c_array) \n",
    "pred = clf.predict(c_array) \n",
    "\n",
    "coef_matrix_cluster4['atm_cluster_id'] = pred\n",
    "coef_matrix_cluster4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, )\n",
    "colors = ['Red', 'Blue', 'Green']\n",
    " \n",
    "for t in range(num_clusters):\n",
    "    x = coef_matrix_cluster4.loc[coef_matrix_cluster4['atm_cluster_id'] == t, 'salary25th']\n",
    "    y = coef_matrix_cluster4.loc[coef_matrix_cluster4['atm_cluster_id'] == t, 'intercept']\n",
    " \n",
    "    ax.scatter(x, y, alpha=0.5, label='cluster ' + str(t), color=colors[t])\n",
    " \n",
    "ax.set_title('Scatter Plot')\n",
    "ax.set_xlabel('salary25th')\n",
    "ax.set_ylabel('intercept')\n",
    " \n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#5回目グルーピング\n",
    "coef_matrix_cluster5 = coef_matrix2[['intercept' ,'日降水量(mm)']]\n",
    "coef_matrix_cluster5\n",
    "\n",
    "c_array= np.array([\n",
    "    coef_matrix_cluster5['intercept'].tolist(),\n",
    "    coef_matrix_cluster5['日降水量(mm)'].tolist()\n",
    "    ], np.float)\n",
    "\n",
    "c_array = c_array.T\n",
    "\n",
    "num_clusters = 3\n",
    "clf = KMeans(n_clusters = num_clusters ) \n",
    "clf.fit(c_array) \n",
    "pred = clf.predict(c_array) \n",
    "\n",
    "coef_matrix_cluster5['atm_cluster_id'] = pred\n",
    "coef_matrix_cluster5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "colors = ['Red', 'Blue', 'Green']\n",
    " \n",
    "for t in range(num_clusters):\n",
    "    x = coef_matrix_cluster5.loc[coef_matrix_cluster5['atm_cluster_id'] == t, '日降水量(mm)']\n",
    "    y = coef_matrix_cluster5.loc[coef_matrix_cluster5['atm_cluster_id'] == t, 'intercept']\n",
    " \n",
    "    ax.scatter(x, y, alpha=0.5, label='cluster ' + str(t), color=colors[t])\n",
    " \n",
    "ax.set_title('Scatter Plot')\n",
    "ax.set_xlabel('日降水量(mm)')\n",
    "ax.set_ylabel('intercept')\n",
    " \n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6回目グルーピング\n",
    "coef_matrix_cluster6 = coef_matrix2[['intercept' ,'day' ,'week']]\n",
    "coef_matrix_cluster6\n",
    "\n",
    "c_array= np.array([\n",
    "    coef_matrix_cluster6['intercept'].tolist(),\n",
    "    coef_matrix_cluster6['day'].tolist(),\n",
    "    coef_matrix_cluster6['week'].tolist()\n",
    "    ], np.float)\n",
    "\n",
    "c_array = c_array.T\n",
    "\n",
    "num_clusters = 3\n",
    "clf = KMeans(n_clusters = num_clusters ) \n",
    "clf.fit(c_array) \n",
    "pred = clf.predict(c_array) \n",
    "\n",
    "coef_matrix_cluster6['atm_cluster_id'] = pred\n",
    "coef_matrix_cluster6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colors = ['Red', 'Blue', 'Green']\n",
    " \n",
    "for t in range(num_clusters):\n",
    "    x = coef_matrix_cluster6.loc[coef_matrix_cluster6['atm_cluster_id'] == t, 'week']\n",
    "    y = coef_matrix_cluster6.loc[coef_matrix_cluster6['atm_cluster_id'] == t, 'intercept']\n",
    "    z = coef_matrix_cluster6.loc[coef_matrix_cluster6['atm_cluster_id'] == t, 'day'] \n",
    "\n",
    "    ax.scatter(x, y, alpha=0.5, label='cluster ' + str(t), color=colors[t])\n",
    " \n",
    "ax.set_title('Scatter Plot')\n",
    "ax.set_xlabel('week')\n",
    "ax.set_ylabel('intercept')\n",
    "ax.set_zlabel('day')\n",
    " \n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
